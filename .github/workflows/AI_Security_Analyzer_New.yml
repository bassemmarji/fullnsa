name: AI-Powered Data Model Analyzer (Enhanced Tabular)

on:
  workflow_dispatch:
    inputs:
      org_alias:
        description: "Salesforce org alias"
        required: true
        default: "Full-NSA"
      model_name:
        description: "Ollama model to use"
        required: true
        default: "llama3:latest"

env:
  NODE_VERSION: "20"
  DATA_DIR: "data"

jobs:
  collect-data:
    name: Collect Salesforce Metadata
    runs-on: ubuntu-latest
    outputs:
      has_data: ${{ steps.check.outputs.has_data }}
    steps:
      - uses: actions/checkout@v4

      - name: Setup Node + Salesforce CLI
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}

      - name: Install Salesforce CLI + jq
        run: |
          sudo apt-get update -y
          sudo apt-get install -y jq
          npm install -g @salesforce/cli

      - name: Authenticate to Salesforce
        run: |
          echo "${{ secrets.ORG_SFDX_URL }}" | sf org login sfdx-url \
            --alias "${{ github.event.inputs.org_alias }}" \
            --sfdx-url-stdin

      - name: Test Tooling API Access
        run: |
          echo "🔍 Testing post-auth Tooling access..."
          sf data query --use-tooling-api --target-org "${{ github.event.inputs.org_alias }}" \
            --query "SELECT count() FROM EntityDefinition LIMIT 1" \
            --result-format human || echo "❌ Quick test failed - check permissions"
          echo "📊 Full EntityDefinition count (for reference):"
          sf data query --use-tooling-api --target-org "${{ github.event.inputs.org_alias }}" \
            --query "SELECT count() FROM EntityDefinition" \
            --result-format human || echo "❌ Full count test failed"
            
      - name: "🔍 Query Salesforce Metadata"
        shell: bash
        run: |
          set -e
          echo "🚀 Starting metadata extraction..."
          mkdir -p "$DATA_DIR"
      
          echo "🔑 Checking Salesforce org authentication..."
          if ! sf org display --target-org "${{ github.event.inputs.org_alias }}" > /dev/null 2>&1; then
            echo "❌ Org alias not found or not authenticated"
            exit 1
          fi
          echo "✅ Org authentication verified."
      
          echo "📦 Querying custom objects..."
          if ! sf data query \
            --use-tooling-api \
            --target-org "${{ github.event.inputs.org_alias }}" \
            --result-format json \
            --query "SELECT Id, DeveloperName, Label, NamespacePrefix, ExternalSharingModel, InternalSharingModel
                     FROM EntityDefinition
                     WHERE DeveloperName LIKE '%__c'
                     LIMIT 2000" > "$DATA_DIR/objects.json" 2>/tmp/query_objects.log; then
              echo "❌ Failed to fetch custom objects"; cat /tmp/query_objects.log; exit 1
          fi
      
          object_count=$(jq -r '.result.records | length' "$DATA_DIR/objects.json")
          echo "✅ Retrieved $object_count custom objects."
          if [ "$object_count" -eq 0 ]; then
            echo "⚠️ No custom objects found. Exiting safely."
            exit 0
          fi
      
          object_names_quoted=$(jq -r '.result.records[].DeveloperName' "$DATA_DIR/objects.json" | sed "s/.*/'&'/g" | paste -sd, -)
          echo "🔹 Objects prepared for queries: $object_names_quoted"
      
          echo "🧩 Querying field definitions..."
          if ! sf data query \
            --use-tooling-api \
            --target-org "${{ github.event.inputs.org_alias }}" \
            --result-format json \
            --query "SELECT QualifiedApiName, EntityDefinition.DeveloperName, DataType, IsIndexed, IsCalculated, IsNillable, Length, Precision, Scale, ReferenceTo, Label
                     FROM FieldDefinition
                     WHERE EntityDefinition.DeveloperName IN ($object_names_quoted)
                     LIMIT 5000" > "$DATA_DIR/fields.json" 2>/tmp/query_fields.log; then
              echo "❌ Failed to fetch fields"; cat /tmp/query_fields.log; exit 1
          fi
          field_count=$(jq -r '.result.records | length' "$DATA_DIR/fields.json")
          echo "✅ Retrieved $field_count fields."
      
          echo "🔗 Extracting relationships..."
          jq '.result.records
              | map(select(.DataType == "Lookup" or .DataType == "MasterDetail"))
              | map({field: .QualifiedApiName, parent: .EntityDefinition.DeveloperName, referencedObject: .ReferenceTo, dataType: .DataType, isIndexed: .IsIndexed})
              ' "$DATA_DIR/fields.json" > "$DATA_DIR/relationships.json"
          rel_count=$(jq -r 'length' "$DATA_DIR/relationships.json")
          echo "✅ Extracted $rel_count relationship fields."
      
          echo "🧠 Querying validation rules..."
          if ! sf data query \
            --use-tooling-api \
            --target-org "${{ github.event.inputs.org_alias }}" \
            --result-format json \
            --query "SELECT ValidationName, EntityDefinition.DeveloperName, Active, ErrorMessage, Description
                     FROM ValidationRule
                     WHERE Active = true
                     AND EntityDefinition.DeveloperName IN ($object_names_quoted)
                     LIMIT 2000" > "$DATA_DIR/validation_rules.json" 2>/tmp/query_vrules.log; then
              echo "❌ Failed to fetch validation rules"; cat /tmp/query_vrules.log; exit 1
          fi
          vrule_count=$(jq -r '.result.records | length' "$DATA_DIR/validation_rules.json")
          echo "✅ Retrieved $vrule_count validation rules."
      
          echo "⚙️ Querying triggers..."
          if ! sf data query \
            --use-tooling-api \
            --target-org "${{ github.event.inputs.org_alias }}" \
            --result-format json \
            --query "SELECT Name, TableEnumOrId, Status, CreatedDate, LastModifiedDate
                     FROM ApexTrigger
                     WHERE TableEnumOrId IN ($object_names_quoted)
                     LIMIT 2000" > "$DATA_DIR/triggers.json" 2>/tmp/query_triggers.log; then
              echo "❌ Failed to fetch triggers"; cat /tmp/query_triggers.log; exit 1
          fi
          trigger_count=$(jq -r '.result.records | length' "$DATA_DIR/triggers.json")
          echo "✅ Retrieved $trigger_count triggers."
      
          echo "🗂 Querying record types..."
          if ! sf data query \
            --use-tooling-api \
            --target-org "${{ github.event.inputs.org_alias }}" \
            --result-format json \
            --query "SELECT Name, SobjectType, Description, IsActive
                     FROM RecordType
                     WHERE SobjectType IN ($object_names_quoted)
                     LIMIT 2000" > "$DATA_DIR/record_types.json" 2>/tmp/query_recordtypes.log; then
              echo "❌ Failed to fetch record types"; cat /tmp/query_recordtypes.log; exit 1
          fi
          rt_count=$(jq -r '.result.records | length' "$DATA_DIR/record_types.json")
          echo "✅ Retrieved $rt_count record types."
      
          echo "🌊 Querying active flows..."
          if ! sf data query \
            --use-tooling-api \
            --target-org "${{ github.event.inputs.org_alias }}" \
            --result-format json \
            --query "SELECT MasterLabel, Definition.DeveloperName, Description, Status, ProcessType, CreatedDate, LastModifiedDate
                     FROM Flow
                     WHERE Status = 'Active'
                     LIMIT 2000" > "$DATA_DIR/flows.json" 2>/tmp/query_flows.log; then
              echo "❌ Failed to fetch flows"; cat /tmp/query_flows.log; exit 1
          fi
          flow_count=$(jq -r '.result.records | length' "$DATA_DIR/flows.json")
          echo "✅ Retrieved $flow_count active flows."
      
          echo "🎉 Metadata extraction completed successfully."
                    
      - name: Validate Data Availability
        id: check
        run: |
          object_count=$(jq -r '.result.totalSize // .totalSize // 0' "$DATA_DIR/objects.json")
          field_count=$(jq -r '.result.totalSize // .totalSize // 0' "$DATA_DIR/fields.json")
          object_len=$(jq -r '.result.records // .records // [] | length' "$DATA_DIR/objects.json")
          field_len=$(jq -r '.result.records // .records // [] | length' "$DATA_DIR/fields.json")
          echo "Objects: count=$object_count (len=$object_len), Fields: count=$field_count (len=$field_len)"
          if [ "$object_count" -gt 0 ] && [ "$field_count" -gt 0 ]; then
            echo "✅ Data model metadata available"
            echo "has_data=true" >> $GITHUB_OUTPUT
          else
            echo "⚠️ No metadata found"
            echo "has_data=false" >> $GITHUB_OUTPUT
          fi

      - uses: actions/upload-artifact@v4
        if: always()
        with:
          name: data-model-metadata-lite
          path: ${{ env.DATA_DIR }}
          retention-days: 7

  analyze-ai:
    name: Analyze Data Model (AI)
    runs-on: ubuntu-latest
    needs: collect-data
    if: needs.collect-data.outputs.has_data == 'true'
    steps:
      - uses: actions/download-artifact@v4
        with:
          name: data-model-metadata-lite
          path: ${{ env.DATA_DIR }}

      - name: Setup Ollama
        run: |
          curl -fsSL https://ollama.com/install.sh | sh
          ollama serve &
          for i in {1..20}; do
            if curl -s http://localhost:11434/api/version >/dev/null; then
              echo "✅ Ollama API ready"
              break
            fi
            echo "⏳ Waiting for Ollama ($i/20)..."
            sleep 3
          done
          if [ $i -eq 20 ]; then
            echo "❌ Ollama failed to start"
            exit 1
          fi
          ollama pull "${{ github.event.inputs.model_name }}"

      - name: "📊 Pre-Analysis: Generate Structured Data"
        shell: bash
        run: |
          echo "📊 Performing pre-analysis to generate structured findings..."
          
          # Helper function
          safe_records() {
            file=$1
            jq 'if .result? and .result.records? then .result.records
                else if .records? then .records
                else if type=="array" then .
                else [] end end end' "$file"
          }
          
          # 1. Missing Indexes Analysis
          echo "🔍 Analyzing missing indexes..."
          jq -r '
            def safe_records: if .result? and .result.records? then .result.records else if .records? then .records else if type=="array" then . else [] end end end;
            
            safe_records |
            map(select(
              (.DataType == "Text" or .DataType == "Number" or .DataType == "Lookup" or .DataType == "MasterDetail" or .DataType == "Email" or .DataType == "Phone") 
              and (.IsIndexed != true)
            )) |
            map({
              object: .EntityDefinition.DeveloperName,
              field: .QualifiedApiName,
              dataType: .DataType,
              label: .Label,
              risk: (if .DataType == "Lookup" or .DataType == "MasterDetail" then "HIGH" 
                     elif .DataType == "Text" and (.Length // 0) > 255 then "MEDIUM"
                     elif .DataType == "Email" or .DataType == "Phone" then "MEDIUM"
                     else "LOW" end)
            }) |
            sort_by(.risk) | reverse
          ' "$DATA_DIR/fields.json" > "$DATA_DIR/missing_indexes_structured.json"
          
          # 2. Duplicate Fields Analysis
          echo "🔍 Analyzing duplicate fields..."
          jq -r '
            def safe_records: if .result? and .result.records? then .result.records else if .records? then .records else if type=="array" then . else [] end end end;
            
            safe_records |
            group_by(.QualifiedApiName) |
            map(select(length > 1)) |
            map({
              fieldName: .[0].QualifiedApiName,
              occurrences: length,
              objects: map(.EntityDefinition.DeveloperName),
              risk: (if length > 3 then "HIGH" elif length > 2 then "MEDIUM" else "LOW" end)
            }) |
            sort_by(.occurrences) | reverse
          ' "$DATA_DIR/fields.json" > "$DATA_DIR/duplicate_fields_structured.json"
          
          # 3. Invalid/Risky Relationships
          echo "🔍 Analyzing relationships..."
          jq -r '
            map({
              parent: .parent,
              field: .field,
              referenced: (.referencedObject // "null"),
              type: .dataType,
              indexed: (.isIndexed // false),
              risk: (if (.referencedObject // "null") == "null" then "HIGH"
                     elif .dataType == "MasterDetail" and (.isIndexed == false) then "HIGH"
                     elif .dataType == "Lookup" and (.isIndexed == false) then "MEDIUM"
                     else "LOW" end)
            }) |
            sort_by(.risk) | reverse
          ' "$DATA_DIR/relationships.json" > "$DATA_DIR/relationships_structured.json"
          
          # 4. Calculate statistics using jq for everything
          echo "📊 Calculating statistics..."
          jq -n \
            --slurpfile fields "$DATA_DIR/fields.json" \
            --slurpfile missing_idx "$DATA_DIR/missing_indexes_structured.json" \
            --slurpfile dup_fields "$DATA_DIR/duplicate_fields_structured.json" \
            --slurpfile relationships "$DATA_DIR/relationships_structured.json" \
            '
            # Extract records safely
            ($fields | map(if .[].result?.records? then .[].result.records else if .[].records? then .[].records else .[] end end) | flatten | length) as $total_fields |
            ($missing_idx | flatten | length) as $missing_indexes |
            ($dup_fields | flatten | length) as $duplicate_fields |
            ($relationships | flatten | length) as $total_relationships |
            ($relationships | flatten | map(select(.risk == "HIGH" or .risk == "MEDIUM")) | length) as $risky_relationships |
            
            # Calculate percentages
            (if $total_fields > 0 then (($missing_indexes * 100.0) / $total_fields) else 0 end) as $missing_idx_pct |
            (if $total_fields > 0 then (($duplicate_fields * 100.0) / $total_fields) else 0 end) as $dup_fields_pct |
            (if $total_relationships > 0 then (($risky_relationships * 100.0) / $total_relationships) else 0 end) as $risky_rel_pct |
            
            {
              totalFields: $total_fields,
              missingIndexes: $missing_indexes,
              missingIndexesPct: ($missing_idx_pct | tonumber | . * 10 | round / 10),
              duplicateFields: $duplicate_fields,
              duplicateFieldsPct: ($dup_fields_pct | tonumber | . * 10 | round / 10),
              totalRelationships: $total_relationships,
              riskyRelationships: $risky_relationships,
              riskyRelationshipsPct: ($risky_rel_pct | tonumber | . * 10 | round / 10)
            }
            ' > "$DATA_DIR/statistics.json"
          
          echo "✅ Pre-analysis complete"
          echo "📊 Statistics:"
          cat "$DATA_DIR/statistics.json" | jq .
          
          echo ""
          echo "📋 File Summary:"
          echo "- Missing Indexes: $(jq length "$DATA_DIR/missing_indexes_structured.json") records"
          echo "- Duplicate Fields: $(jq length "$DATA_DIR/duplicate_fields_structured.json") records"
          echo "- Relationships: $(jq length "$DATA_DIR/relationships_structured.json") records"

      - name: "🧠 AI Analysis with Structured Output"
        shell: bash
        run: |
          echo "🧠 Starting AI Analysis with structured prompting..."
          REPORT_FILE="data_model_analysis_$(date +%Y%m%d_%H%M%S).md"
          MODEL="${{ github.event.inputs.model_name }}"
          
          # Read statistics
          stats=$(cat "$DATA_DIR/statistics.json")
          
          # Create structured prompt
          SYSTEM_PROMPT='You are a Salesforce Data Architect. Provide analysis in MARKDOWN TABLE FORMAT ONLY. Be concise and data-driven. Follow the exact structure requested.'
          
          USER_PROMPT=$(cat << 'EOFPROMPT'
          Analyze this Salesforce metadata and respond with ONLY markdown tables in the exact format shown below.

          ## 📊 Executive Summary

          | Metric | Count | Risk % | Status |
          |--------|-------|--------|--------|
          | Missing Indexes | X | Y% | 🔴/🟡/🟢 |
          | Duplicate Fields | X | Y% | 🔴/🟡/🟢 |
          | Risky Relationships | X | Y% | 🔴/🟡/🟢 |

          Risk Status: 🔴 HIGH (>20%), 🟡 MEDIUM (10-20%), 🟢 LOW (<10%)

          ## 🔍 Missing Indexes (Top 10)

          | Object | Field | Data Type | Risk Level | Impact |
          |--------|-------|-----------|------------|--------|
          | Object__c | Field__c | Lookup | HIGH | Query performance |

          ## 🧩 Duplicate Fields (Top 10)

          | Field Name | Occurrences | Objects | Risk Level | Recommendation |
          |------------|-------------|---------|------------|----------------|
          | Name | 3 | Obj1, Obj2, Obj3 | MEDIUM | Consolidate |

          ## 🔗 Invalid/Risky Relationships (Top 10)

          | Parent Object | Field | Referenced Object | Type | Indexed | Risk | Issue |
          |---------------|-------|-------------------|------|---------|------|-------|
          | Order__c | Account__c | null | Lookup | No | HIGH | Broken reference |

          ## ✅ Recommendations

          Provide 3-5 prioritized recommendations as a bullet list.

          Use ONLY the data provided below. Do NOT add explanatory text outside tables.

          ### STATISTICS:
EOFPROMPT
          )
          
          # Combine into single prompt
          cat > prompt.txt << EOF
          $USER_PROMPT
          
          $stats
          
          ### MISSING INDEXES (Sample):
          $(jq -c '[limit(15; .[])]' "$DATA_DIR/missing_indexes_structured.json")
          
          ### DUPLICATE FIELDS (Sample):
          $(jq -c '[limit(15; .[])]' "$DATA_DIR/duplicate_fields_structured.json")
          
          ### RELATIONSHIPS (Sample):
          $(jq -c '[limit(15; .[])]' "$DATA_DIR/relationships_structured.json")
          EOF
          
          # Send to Ollama
          cat > request.json << EOF
          {
            "model": "$MODEL",
            "prompt": $(jq -Rs . < prompt.txt),
            "system": "$SYSTEM_PROMPT",
            "options": {
              "temperature": 0.1,
              "top_p": 0.9,
              "num_predict": 2000
            },
            "stream": false
          }
          EOF
          
          echo "📤 Sending request to Ollama..."
          ollama_response=$(curl -s http://localhost:11434/api/generate \
            -H "Content-Type: application/json" \
            -d @request.json | jq -r '.response // empty')
          
          if [ -z "$ollama_response" ]; then
            echo "❌ Ollama returned empty response"
            exit 1
          fi
          
          # Generate report
          cat > "$REPORT_FILE" << EOF
          # 📊 Salesforce Data Model Analysis Report
          
          **Model:** $MODEL  
          **Generated:** $(date)  
          **Org:** ${{ github.event.inputs.org_alias }}
          
          ---
          
          $ollama_response
          
          ---
          
          ## 📁 Raw Data Files
          
          - \`missing_indexes_structured.json\` - All fields missing indexes
          - \`duplicate_fields_structured.json\` - All duplicate field patterns
          - \`relationships_structured.json\` - All relationship configurations
          - \`statistics.json\` - Overall metrics
          
          **Analysis powered by Ollama ($MODEL)**
          EOF
          
          echo "✅ AI analysis completed"
          cat "$REPORT_FILE"
          
          # Cleanup
          rm -f request.json prompt.txt

      - uses: actions/upload-artifact@v4
        with:
          name: ai-data-model-report
          path: |
            data_model_analysis_*.md
            ${{ env.DATA_DIR }}/*_structured.json
            ${{ env.DATA_DIR }}/statistics.json
          retention-days: 14

      - name: 📄 Generate GitHub Summary
        run: |
          REPORT=$(ls -t data_model_analysis_*.md | head -1)
          
          echo "# 🧠 AI Data Model Analysis Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Model:** \`${{ github.event.inputs.model_name }}\`" >> $GITHUB_STEP_SUMMARY
          echo "**Org:** \`${{ github.event.inputs.org_alias }}\`" >> $GITHUB_STEP_SUMMARY
          echo "**Generated:** $(date)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Include statistics
          if [ -f "$DATA_DIR/statistics.json" ]; then
            echo "## 📊 Quick Stats" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "| Metric | Value |" >> $GITHUB_STEP_SUMMARY
            echo "|--------|-------|" >> $GITHUB_STEP_SUMMARY
            echo "| Total Fields | $(jq -r .totalFields $DATA_DIR/statistics.json) |" >> $GITHUB_STEP_SUMMARY
            echo "| Missing Indexes | $(jq -r .missingIndexes $DATA_DIR/statistics.json) ($(jq -r .missingIndexesPct $DATA_DIR/statistics.json)%) |" >> $GITHUB_STEP_SUMMARY
            echo "| Duplicate Fields | $(jq -r .duplicateFields $DATA_DIR/statistics.json) ($(jq -r .duplicateFieldsPct $DATA_DIR/statistics.json)%) |" >> $GITHUB_STEP_SUMMARY
            echo "| Risky Relationships | $(jq -r .riskyRelationships $DATA_DIR/statistics.json)/$(jq -r .totalRelationships $DATA_DIR/statistics.json) ($(jq -r .riskyRelationshipsPct $DATA_DIR/statistics.json)%) |" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
          fi
          
          echo "## 📄 Full Report" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Download the complete analysis report from the artifacts." >> $GITHUB_STEP_SUMMARY
