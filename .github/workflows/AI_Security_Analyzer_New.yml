name: AI-Powered New Data Model Analyzer (Lite-Safe Enhanced)

on:
  workflow_dispatch:
    inputs:
      org_alias:
        description: "Salesforce org alias"
        required: true
        default: "dev"
      model_name:
        description: "Ollama model to use"
        required: true
        default: "llama3:latest"
      query_limit:
        description: "Max records per query (default 2000; set 0 for no limit)"
        required: false
        default: "2000"
      notify_on_failure:
        description: "Create GitHub issue on failure?"
        required: false
        type: boolean
        default: true

env:
  NODE_VERSION: "20"
  DATA_DIR: "data-model-data"

jobs:
  collect-data:
    name: Collect Salesforce Metadata
    runs-on: ubuntu-latest
    outputs:
      has_data: ${{ steps.check.outputs.has_data }}
    steps:
      - uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}

      # Enhanced: Use official SF CLI action for reliability
      - name: Install Salesforce CLI + jq
        uses: sfdx-actions/setup-sfdx@v1
        with:
          sfdx-version: latest

      - name: Install jq
        run: sudo apt-get update -y && sudo apt-get install -y jq

      - name: Authenticate to Salesforce
        run: |
          echo "${{ secrets.ORG_SFDX_URL }}" | sf org login sfdx-url \
            --alias "${{ github.event.inputs.org_alias }}" \
            --sfdx-url-stdin
        shell: bash

      - name: Query Salesforce Metadata (Safe Mode, Parallel)
        run: |
          mkdir -p "$DATA_DIR"
          LIMIT="${{ github.event.inputs.query_limit }}"
          LIMIT_OPT=""
          [ "$LIMIT" != "0" ] && LIMIT_OPT="LIMIT $LIMIT"

          # Enhanced: Parallelize queries with background jobs for speed
          echo "ðŸ” Querying EntityDefinition..."
          sf data query --use-tooling-api --target-org "${{ github.event.inputs.org_alias }}" \
            --result-format json \
            --query "SELECT DeveloperName, Label, NamespacePrefix, ExternalSharingModel, InternalSharingModel 
                     FROM EntityDefinition 
                     WHERE IsCustomizable = true 
                     $LIMIT_OPT" > "$DATA_DIR/objects.json" || echo '{"result":{"records":[]}}' > "$DATA_DIR/objects.json" &

          echo "ðŸ” Querying FieldDefinition..."
          sf data query --use-tooling-api --target-org "${{ github.event.inputs.org_alias }}" \
            --result-format json \
            --query "SELECT QualifiedApiName, EntityDefinition.DeveloperName, DataType, IsIndexed, IsCalculated, IsNillable, Length, Precision, Scale 
                     FROM FieldDefinition 
                     WHERE EntityDefinition.IsCustomizable = true 
                     $LIMIT_OPT" > "$DATA_DIR/fields.json" || echo '{"result":{"records":[]}}' > "$DATA_DIR/fields.json" &

          echo "ðŸ” Querying Relationships..."
          sf data query --use-tooling-api --target-org "${{ github.event.inputs.org_alias }}" \
            --result-format json \
            --query "SELECT QualifiedApiName, EntityDefinition.DeveloperName, RelationshipName, ReferenceTo 
                     FROM FieldDefinition 
                     WHERE DataType IN ('Lookup','MasterDetail') 
                     $LIMIT_OPT" > "$DATA_DIR/relationships.json" || echo '{"result":{"records":[]}}' > "$DATA_DIR/relationships.json" &

          echo "ðŸ” Querying Validation Rules..."
          sf data query --use-tooling-api --target-org "${{ github.event.inputs.org_alias }}" \
            --result-format json \
            --query "SELECT ValidationName, EntityDefinition.DeveloperName, Active, ErrorMessage, Description 
                     FROM ValidationRule 
                     WHERE Active = true 
                     $LIMIT_OPT" > "$DATA_DIR/validation_rules.json" || echo '{"result":{"records":[]}}' > "$DATA_DIR/validation_rules.json" &

          echo "ðŸ¤– Querying Apex Triggers..."
          sf data query --use-tooling-api --target-org "${{ github.event.inputs.org_alias }}" \
            --result-format json \
            --query "SELECT Name, TableEnumOrId, Status, CreatedDate, LastModifiedDate 
                     FROM ApexTrigger 
                     $LIMIT_OPT" > "$DATA_DIR/triggers.json" || echo '{"result":{"records":[]}}' > "$DATA_DIR/triggers.json" &

          echo "âš™ï¸ Querying Flow Definitions..."
          sf data query --use-tooling-api --target-org "${{ github.event.inputs.org_alias }}" \
            --result-format json \
            --query "SELECT DeveloperName, Description, Status, ProcessType, CreatedDate, LastModifiedDate 
                     FROM FlowDefinition 
                     $LIMIT_OPT" > "$DATA_DIR/flows.json" || echo '{"result":{"records":[]}}' > "$DATA_DIR/flows.json" &

          echo "ðŸ“‹ Querying Record Types..."
          sf data query --use-tooling-api --target-org "${{ github.event.inputs.org_alias }}" \
            --result-format json \
            --query "SELECT DeveloperName, SobjectType, Name, Description, IsActive 
                     FROM RecordType 
                     $LIMIT_OPT" > "$DATA_DIR/record_types.json" || echo '{"result":{"records":[]}}' > "$DATA_DIR/record_types.json" &

          # Wait for all background jobs
          wait
        shell: bash

      - name: Validate Data Availability
        id: check
        run: |
          object_count=$(jq -r '.result.totalSize // 0' "$DATA_DIR/objects.json")
          field_count=$(jq -r '.result.totalSize // 0' "$DATA_DIR/fields.json")
          echo "Objects: $object_count, Fields: $field_count"

          if [ "$object_count" -gt 0 ] && [ "$field_count" -gt 0 ]; then
            echo "âœ… Data model metadata available"
            echo "has_data=true" >> $GITHUB_OUTPUT
          else
            echo "âš ï¸ No metadata found"
            echo "has_data=false" >> $GITHUB_OUTPUT
          fi
        shell: bash

      # Enhanced: Cleanup sensitive files
      - name: Cleanup Temp Files
        if: always()
        run: |
          rm -rf ~/.sfdx || true
        shell: bash

      - uses: actions/upload-artifact@v4
        if: steps.check.outputs.has_data == 'true'
        with:
          name: data-model-metadata-lite
          path: ${{ env.DATA_DIR }}
          retention-days: 7

  analyze-ai:
    name: Analyze Data Model (AI)
    runs-on: ubuntu-latest
    needs: collect-data
    if: needs.collect-data.outputs.has_data == 'true'
    steps:
      - uses: actions/download-artifact@v4
        with:
          name: data-model-metadata-lite
          path: data

      # Enhanced: Cache Ollama models for faster pulls
      - name: Setup Ollama with Cache
        uses: actions/cache@v4
        with:
          path: ~/.ollama/models
          key: ollama-models-${{ runner.os }}-${{ github.event.inputs.model_name }}
          restore-keys: ollama-models-${{ runner.os }}-

      - name: Install and Start Ollama
        run: |
          curl -fsSL https://ollama.com/install.sh | sh
          ollama serve &
          for i in {1..20}; do
            curl -s http://localhost:11434/api/version && break || sleep 2
          done
          ollama pull "${{ github.event.inputs.model_name }}" || echo "Model pull failed, but continuing..."
        shell: bash

      - name: Analyze Data Model with AI
        run: |
          REPORT_FILE="data_model_analysis.md"
          MODEL="${{ github.event.inputs.model_name }}"
          echo "# Salesforce Data Model Analysis Report" > "$REPORT_FILE"
          echo "**Model:** $MODEL" >> "$REPORT_FILE"
          echo "**Generated:** $(date)" >> "$REPORT_FILE"
          echo "" >> "$REPORT_FILE"

          # Enhanced: Combine ALL JSONs for comprehensive analysis
          jq -s '{
            objects: .[0].result.records,
            fields: .[1].result.records,
            relationships: .[2].result.records,
            validation_rules: .[3].result.records,
            triggers: .[4].result.records,
            flows: .[5].result.records,
            record_types: .[6].result.records
          }' \
          data/objects.json data/fields.json data/relationships.json data/validation_rules.json \
          data/triggers.json data/flows.json data/record_types.json > data/combined.json

          # Enhanced: Structured prompt for consistent output
          prompt="You are a senior Salesforce Data Architect. Analyze the provided JSON metadata for custom objects, fields, relationships, validation rules, triggers, flows, and record types. 
          Output in Markdown with these sections: 
          1. **Overview**: Key stats (e.g., # objects, avg fields/object, # relationships). 
          2. **Normalization & Design**: Assess 1NF/2NF/3NF compliance, redundancy risks. 
          3. **Relationships & Integrity**: Cardinality, sharing models, potential cycles/orphans. 
          4. **Performance & Indexing**: Indexed fields, calculated fields, nillable risks. 
          5. **Constraints & Automation**: Validation rules, triggers, flows impacting data quality. 
          6. **Recommendations**: 3-5 prioritized fixes with rationale. 
          Be concise, use tables for lists, focus on issues > opportunities."

          # Enhanced: Timeout and error handling for Ollama
          timeout 300s ollama run "$MODEL" "$prompt" < data/combined.json >> "$REPORT_FILE" || {
            echo "âŒ AI analysis failed - appending fallback summary" >> "$REPORT_FILE"
            echo "## Fallback Summary\nAnalysis timed out. Review raw metadata manually." >> "$REPORT_FILE"
          }

          echo "" >> "$REPORT_FILE"
          echo "---" >> "$REPORT_FILE"
          echo "## Technical Summary" >> "$REPORT_FILE"
          echo "- Objects: $(jq -r '.result.totalSize // 0' data/objects.json)" >> "$REPORT_FILE"
          echo "- Fields: $(jq -r '.result.totalSize // 0' data/fields.json)" >> "$REPORT_FILE"
          # Enhanced: Add simple metric (e.g., avg fields per object)
          objects=$(jq -r '.result.totalSize // 1' data/objects.json)
          fields=$(jq -r '.result.totalSize // 0' data/fields.json)
          avg_fields=$((fields / objects))
          echo "- Avg Fields/Object: $avg_fields" >> "$REPORT_FILE"
          echo "- Generated: $(date)" >> "$REPORT_FILE"
          
          # Enhanced: Cleanup
          rm data/combined.json || true
        shell: bash

      - uses: actions/upload-artifact@v4
        with:
          name: ai-data-model-report
          path: data_model_analysis.md
          retention-days: 14

      - name: ðŸ“„ AI Summary
        run: |
          echo "## ðŸ§  AI Data Model Summary" >> $GITHUB_STEP_SUMMARY
          echo "- Model: \`${{ github.event.inputs.model_name }}\`" >> $GITHUB_STEP_SUMMARY
          echo "- Report: \`data_model_analysis.md\`" >> $GITHUB_STEP_SUMMARY
          echo "- Artifact: \`ai-data-model-report\`" >> $GITHUB_STEP_SUMMARY

      # Enhanced: More robust summary extraction (first 200 chars of report body)
      - name: ðŸ§© AI Report Quick Summary
        run: |
          summary=$(head -200 data_model_analysis.md | tail -n +3 | tr '\n' ' ' | cut -c1-200)
          echo "**Quick Summary:** $summary..." >> "$GITHUB_STEP_SUMMARY"
        shell: bash

      # Enhanced: Notify on failure
      - name: Notify on Failure
        if: failure() && github.event.inputs.notify_on_failure == 'true'
        uses: actions/github-script@v7
        with:
          script: |
            github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: 'Data Model Analysis Failed: ${{ github.event.inputs.org_alias }}',
              body: 'Run ID: ${{ github.run_id }}\nModel: ${{ github.event.inputs.model_name }}\nCheck logs for details.'
            })








