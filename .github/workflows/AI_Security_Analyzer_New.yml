name: AI-Powered New Data Model Analyzer (Lite-Safe)

on:
  workflow_dispatch:
    inputs:
      org_alias:
        description: "Salesforce org alias"
        required: true
        default: "Full-NSA"
      model_name:
        description: "Ollama model to use"
        required: true
        default: "llama3:latest"

env:
  NODE_VERSION: "20"
  DATA_DIR: "data"  # Simplified for consistency

jobs:
  collect-data:
    name: Collect Salesforce Metadata
    runs-on: ubuntu-latest
    outputs:
      has_data: ${{ steps.check.outputs.has_data }}
    steps:
      - uses: actions/checkout@v4

      - name: Setup Node + Salesforce CLI
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}

      - name: Install Salesforce CLI + jq
        run: |
          sudo apt-get update -y
          sudo apt-get install -y jq
          npm install -g @salesforce/cli

      - name: Authenticate to Salesforce
        run: |
          echo "${{ secrets.ORG_SFDX_URL }}" | sf org login sfdx-url \
            --alias "${{ github.event.inputs.org_alias }}" \
            --sfdx-url-stdin

      - name: Test Tooling API Access
        run: |
          echo "🔍 Testing post-auth Tooling access..."
          sf data query --use-tooling-api --target-org "${{ github.event.inputs.org_alias }}" \
            --query "SELECT count() FROM EntityDefinition LIMIT 1" \
            --result-format human || echo "❌ Quick test failed - check permissions"
          echo "📊 Full EntityDefinition count (for reference):"
          sf data query --use-tooling-api --target-org "${{ github.event.inputs.org_alias }}" \
            --query "SELECT count() FROM EntityDefinition" \
            --result-format human || echo "❌ Full count test failed"
            
      - name: "🔍 Query Salesforce Metadata (Safe Mode - Custom Objects + Relationships + RecordTypes + Flows)"
        shell: bash
        run: |
          set -e
          echo "🚀 Starting safe metadata extraction (Custom Objects + Relationships + RecordTypes + Flows)..."
          mkdir -p "$DATA_DIR"
      
          # === STEP 0: Verify org authentication ===
          echo "🔑 Checking Salesforce org authentication..."
          if ! sf org display --target-org "${{ github.event.inputs.org_alias }}" > /dev/null 2>&1; then
            echo "❌ Org alias not found or not authenticated: ${{ github.event.inputs.org_alias }}"
            echo "Please ensure the org is authorized using 'sf org login web' or 'sf org login sfdx-url'."
            exit 1
          fi
          echo "✅ Org authentication verified."
      
          # === STEP 1: Fetch Custom Objects Only ===
          echo "📦 Querying custom objects..."
          if ! sf data query \
            --use-tooling-api \
            --target-org "${{ github.event.inputs.org_alias }}" \
            --result-format json \
            --query "SELECT Id, DeveloperName, Label, NamespacePrefix, ExternalSharingModel, InternalSharingModel
                     FROM EntityDefinition
                     WHERE DeveloperName LIKE '%__c'
                     LIMIT 2000" > "$DATA_DIR/objects.json" 2>/tmp/query_objects.log; then
              echo "❌ Failed to fetch custom objects. Log:"; cat /tmp/query_objects.log; exit 1
          fi
      
          object_count=$(jq -r '.result.records | length' "$DATA_DIR/objects.json")
          echo "✅ Retrieved $object_count custom objects."
          if [ "$object_count" -eq 0 ]; then
            echo "⚠️ No custom objects found. Exiting safely."
            exit 0
          fi
      
          # === STEP 2: Prepare Object Names for SOQL IN Clause ===
          object_names_quoted=$(jq -r '.result.records[].DeveloperName' "$DATA_DIR/objects.json" | sed "s/.*/'&'/g" | paste -sd, -)
          echo "🔹 Objects prepared for queries: $object_names_quoted"
      
          # === STEP 3: Query Field Definitions ===
          echo "🧩 Querying field definitions for custom objects..."
          if ! sf data query \
            --use-tooling-api \
            --target-org "${{ github.event.inputs.org_alias }}" \
            --result-format json \
            --query "SELECT QualifiedApiName, EntityDefinition.DeveloperName, DataType, IsIndexed, IsCalculated, IsNillable, Length, Precision, Scale, ReferenceTo
                     FROM FieldDefinition
                     WHERE EntityDefinition.DeveloperName IN ($object_names_quoted)
                     LIMIT 5000" > "$DATA_DIR/fields.json" 2>/tmp/query_fields.log; then
              echo "❌ Failed to fetch fields. Log:"; cat /tmp/query_fields.log; exit 1
          fi
          field_count=$(jq -r '.result.records | length' "$DATA_DIR/fields.json")
          echo "✅ Retrieved $field_count fields."
      
          # === STEP 4: Extract Relationships (Lookup / Master-Detail) ===
          echo "🔗 Extracting relationship fields..."
          jq '.result.records
              | map(select(.DataType == "Lookup" or .DataType == "MasterDetail"))
              | map({field: .QualifiedApiName, parent: .EntityDefinition.DeveloperName, referencedObject: .ReferenceTo})
              ' "$DATA_DIR/fields.json" > "$DATA_DIR/relationships.json"
          rel_count=$(jq -r 'length' "$DATA_DIR/relationships.json")
          echo "✅ Extracted $rel_count relationship fields."
      
          # === STEP 5: Query Validation Rules ===
          echo "🧠 Querying validation rules..."
          if ! sf data query \
            --use-tooling-api \
            --target-org "${{ github.event.inputs.org_alias }}" \
            --result-format json \
            --query "SELECT ValidationName, EntityDefinition.DeveloperName, Active, ErrorMessage, Description
                     FROM ValidationRule
                     WHERE Active = true
                     AND EntityDefinition.DeveloperName IN ($object_names_quoted)
                     LIMIT 2000" > "$DATA_DIR/validation_rules.json" 2>/tmp/query_vrules.log; then
              echo "❌ Failed to fetch validation rules. Log:"; cat /tmp/query_vrules.log; exit 1
          fi
          vrule_count=$(jq -r '.result.records | length' "$DATA_DIR/validation_rules.json")
          echo "✅ Retrieved $vrule_count validation rules."
      
          # === STEP 6: Query Apex Triggers ===
          echo "⚙️ Querying triggers..."
          if ! sf data query \
            --use-tooling-api \
            --target-org "${{ github.event.inputs.org_alias }}" \
            --result-format json \
            --query "SELECT Name, TableEnumOrId, Status, CreatedDate, LastModifiedDate
                     FROM ApexTrigger
                     WHERE TableEnumOrId IN ($object_names_quoted)
                     LIMIT 2000" > "$DATA_DIR/triggers.json" 2>/tmp/query_triggers.log; then
              echo "❌ Failed to fetch triggers. Log:"; cat /tmp/query_triggers.log; exit 1
          fi
          trigger_count=$(jq -r '.result.records | length' "$DATA_DIR/triggers.json")
          echo "✅ Retrieved $trigger_count triggers."
      
          # === STEP 7: Query Record Types ===
          echo "🗂 Querying record types for custom objects..."
          if ! sf data query \
            --use-tooling-api \
            --target-org "${{ github.event.inputs.org_alias }}" \
            --result-format json \
            --query "SELECT Name, SobjectType, Description, IsActive
                     FROM RecordType
                     WHERE SobjectType IN ($object_names_quoted)
                     LIMIT 2000" > "$DATA_DIR/record_types.json" 2>/tmp/query_recordtypes.log; then
              echo "❌ Failed to fetch record types. Log:"; cat /tmp/query_recordtypes.log; exit 1
          fi
          rt_count=$(jq -r '.result.records | length' "$DATA_DIR/record_types.json")
          echo "✅ Retrieved $rt_count record types."
      
          # === STEP 8: Query Active Flows ===
          echo "🌊 Querying active flows..."
          if ! sf data query \
            --use-tooling-api \
            --target-org "${{ github.event.inputs.org_alias }}" \
            --result-format json \
            --query "SELECT MasterLabel, Definition.DeveloperName, Description, Status, ProcessType, CreatedDate, LastModifiedDate
                     FROM Flow
                     WHERE Status = 'Active'
                     LIMIT 2000" > "$DATA_DIR/flows.json" 2>/tmp/query_flows.log; then
              echo "❌ Failed to fetch flows. Log:"; cat /tmp/query_flows.log; exit 1
          fi
          flow_count=$(jq -r '.result.records | length' "$DATA_DIR/flows.json")
          echo "✅ Retrieved $flow_count active flows."
      
          echo "🎉 Metadata extraction (Custom Objects + Relationships + RecordTypes + Flows) completed successfully."
                    
      - name: Validate Data Availability
        id: check
        run: |
          object_count=$(jq -r '.result.totalSize // .totalSize // 0' "$DATA_DIR/objects.json")
          field_count=$(jq -r '.result.totalSize // .totalSize // 0' "$DATA_DIR/fields.json")
          object_len=$(jq -r '.result.records // .records // [] | length' "$DATA_DIR/objects.json")
          field_len=$(jq -r '.result.records // .records // [] | length' "$DATA_DIR/fields.json")
          echo "Objects: count=$object_count (len=$object_len), Fields: count=$field_count (len=$field_len)"
          if [ "$object_count" -gt 0 ] && [ "$field_count" -gt 0 ]; then
            echo "✅ Data model metadata available"
            echo "has_data=true" >> $GITHUB_OUTPUT
          else
            echo "⚠️ No metadata found (check org/permissions)"
            echo "has_data=false" >> $GITHUB_OUTPUT
          fi

      - uses: actions/upload-artifact@v4
        if: always()  # Upload even on failure for debugging
        with:
          name: data-model-metadata-lite
          path: ${{ env.DATA_DIR }}
          retention-days: 7

  analyze-ai:
    name: Analyze Data Model (AI)
    runs-on: ubuntu-latest
    needs: collect-data
    if: needs.collect-data.outputs.has_data == 'true'
    steps:
      - uses: actions/download-artifact@v4
        with:
          name: data-model-metadata-lite
          path: ${{ env.DATA_DIR }}

      - name: Setup Ollama
        run: |
          curl -fsSL https://ollama.com/install.sh | sh
          ollama serve &
          for i in {1..20}; do
            if curl -s http://localhost:11434/api/version >/dev/null; then
              echo "✅ Ollama API ready"
              break
            fi
            echo "⏳ Waiting for Ollama to start ($i/20)..."
            sleep 3
          done
          if [ $i -eq 20 ]; then
            echo "❌ Ollama failed to start"
            exit 1
          fi
          ollama pull "${{ github.event.inputs.model_name }}"

      - name: "🧠 Analyze Data Model with AI (with pre-analysis)"
        shell: bash
        run: |
          echo "🧠 Starting AI Analysis Step with Pre-analysis..."
          REPORT_FILE="data_model_analysis_$(date +%Y%m%d_%H%M%S).md"
          MODEL="${{ github.event.inputs.model_name }}"
      
          echo "🔍 Checking metadata files..."
          ls -lh "$DATA_DIR" || echo "⚠️ No data folder found!"
      
          # Safety: ensure JSON files exist and are readable
          for f in "$DATA_DIR"/*.json; do
            if [ ! -s "$f" ]; then
              echo "⚠️ Missing or empty file: $f"
            else
              echo "✅ Found $f ($(wc -l < "$f") lines)"
            fi
          done
      
          # --- Helper to safely get records from JSON files ---
          safe_records() {
            file=$1
            jq 'if .result? and .result.records? then .result.records
                else if .records? then .records
                else if type=="array" then .
                else [] end end end' "$file"
          }
      
          # --- Pre-count ---
          echo "📊 Metadata counts:"
          echo "- Objects: $(safe_records "$DATA_DIR/objects.json" | jq length)"
          echo "- Fields: $(safe_records "$DATA_DIR/fields.json" | jq length)"
          echo "- Relationships: $(safe_records "$DATA_DIR/relationships.json" | jq length)"
          echo "- Validation Rules: $(safe_records "$DATA_DIR/validation_rules.json" | jq length)"
          echo "- Triggers: $(safe_records "$DATA_DIR/triggers.json" | jq length)"
          echo "- Record Types: $(safe_records "$DATA_DIR/record_types.json" | jq length)"
          echo "- Flows: $(safe_records "$DATA_DIR/flows.json" | jq length)"
      
          echo "🧩 Combining JSON metadata..."
          jq -n \
            --slurpfile objects "$DATA_DIR/objects.json" \
            --slurpfile fields "$DATA_DIR/fields.json" \
            --slurpfile relationships "$DATA_DIR/relationships.json" \
            --slurpfile validation_rules "$DATA_DIR/validation_rules.json" \
            --slurpfile triggers "$DATA_DIR/triggers.json" \
            --slurpfile record_types "$DATA_DIR/record_types.json" \
            --slurpfile flows "$DATA_DIR/flows.json" \
            '{
              objects: ($objects | map(if .result?.records? then .result.records else if .records? then .records else . end end) | flatten),
              fields: ($fields | map(if .result?.records? then .result.records else if .records? then .records else . end end) | flatten),
              relationships: ($relationships | map(if .result?.records? then .result.records else if .records? then .records else . end end) | flatten),
              validation_rules: ($validation_rules | map(if .result?.records? then .result.records else if .records? then .records else . end end) | flatten),
              triggers: ($triggers | map(if .result?.records? then .result.records else if .records? then .records else . end end) | flatten),
              record_types: ($record_types | map(if .result?.records? then .result.records else if .records? then .records else . end end) | flatten),
              flows: ($flows | map(if .result?.records? then .result.records else if .records? then .records else . end end) | flatten)
            }' > "$DATA_DIR/full_metadata.json"
      
          # --- Pre-analysis: Duplicate Fields ---
          echo "🧩 Detecting duplicate fields..."
          jq '[.fields[] | {object: .EntityDefinition.DeveloperName, api: .QualifiedApiName, label: .Label}] |
              group_by(.api) | map(select(length > 1))' "$DATA_DIR/full_metadata.json" > "$DATA_DIR/duplicate_fields.json"
          dup_count=$(jq length "$DATA_DIR/duplicate_fields.json")
          echo "✅ Found $dup_count duplicate fields"
      
          # --- Pre-analysis: Missing Indexes ---
          echo "🧩 Detecting fields likely missing indexes..."
          jq '[.fields[] | select((.DataType=="Text" or .DataType=="Number" or .DataType=="Lookup" or .DataType=="MasterDetail") and (.IsIndexed!=true or .IsIndexed==null))]' \
            "$DATA_DIR/full_metadata.json" > "$DATA_DIR/missing_indexes.json"
          mi_count=$(jq length "$DATA_DIR/missing_indexes.json")
          echo "✅ Found $mi_count potentially missing indexes"
      
          # --- AI Prompt ---
          echo "🧠 Sending prompt to Ollama..."
          SYSTEM_PROMPT='You are a senior Salesforce Data Architect. Be concise and factual. Base every point ONLY on the JSON.'
      
          USER_PROMPT=$(cat << 'EOF'
          You are a Salesforce Data Architect. Analyze the provided Salesforce metadata JSON and pre-analyzed files:
          
          ## 🔍 Missing Indexes
          - Focus on fields in missing_indexes.json
          - Highlight numeric or text fields used in relationships that lack IsIndexed=true
          
          ## 🧩 Duplicate Fields
          - Focus on duplicate_fields.json
          - Identify fields with same QualifiedApiName or Label
          
          ## 🔗 Relationships
          - Describe key Lookup and Master-Detail relationships from relationships.json
          - Identify circular or weakly defined relationships
          
          ## ✅ Recommendations
          - Provide short, factual bullet-point advice based on the above findings
          
          Use bullet points only, no paragraphs. If any section lacks data, write "Insufficient data".
          
          ### METADATA INPUT ###
          EOF
          )
      
          echo "$USER_PROMPT" > prompt.txt
          jq -c '.' "$DATA_DIR/full_metadata.json" >> prompt.txt
          jq -c '.' "$DATA_DIR/duplicate_fields.json" >> prompt.txt
          jq -c '.' "$DATA_DIR/missing_indexes.json" >> prompt.txt
      
          # --- Send to Ollama ---
          cat > request.json << EOF
          {
            "model": "$MODEL",
            "prompt": $(jq -Rs . < prompt.txt),
            "system": "$SYSTEM_PROMPT",
            "options": {"temperature": 0.2, "top_p": 0.9},
            "stream": false
          }
          EOF
      
          ollama_response=$(curl -s http://localhost:11434/api/generate \
            -H "Content-Type: application/json" \
            -d @request.json | jq -r '.response // empty')
      
          if [ -z "$ollama_response" ]; then
            echo "❌ Ollama returned empty response"
            exit 1
          fi
      
          echo "# Salesforce Data Model Analysis Report" > "$REPORT_FILE"
          echo "**Model:** $MODEL" >> "$REPORT_FILE"
          echo "**Generated:** $(date)" >> "$REPORT_FILE"
          echo "" >> "$REPORT_FILE"
          echo "$ollama_response" >> "$REPORT_FILE"
          echo "✅ AI analysis completed successfully"
      
          # Clean up
          rm -f request.json prompt.txt
          
      - uses: actions/upload-artifact@v4
        with:
          name: ai-data-model-report
          path: data_model_analysis_*.md
          retention-days: 14

      - name: 📄 AI Summary
        run: |
          echo "## 🧠 AI Data Model Summary" >> $GITHUB_STEP_SUMMARY
          echo "- Model: \`${{ github.event.inputs.model_name }}\`" >> $GITHUB_STEP_SUMMARY
          echo "- Generated report successfully" >> $GITHUB_STEP_SUMMARY
