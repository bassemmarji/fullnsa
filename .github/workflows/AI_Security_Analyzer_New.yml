name: AI New Powered Profiles and Permission Sets Analyzer

on:
  workflow_dispatch:
    inputs:
      org_alias:
        type: string
        description: "Salesforce org alias"
        required: true
        default: "dev"
      model_name:
        type: choice
        description: "Ollama model to use"
        options:
          - llama3:latest
          - gemma2:27b
          - mistral-large
        default: llama3:latest

env:
  ORG_ALIAS: ${{ github.event.inputs.org_alias }}
  MODEL_NAME: ${{ github.event.inputs.model_name }}
  NODE_VERSION: "20"
  DATA_DIR: "security-profiles-data"

jobs:
  collect-security-data:
    name: Collect Profiles and Permission Sets Data
    runs-on: ubuntu-latest
    outputs:
      has_data: ${{ steps.check-data.outputs.has_data }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}

      - name: Install dependencies
        run: |
          npm install --global @salesforce/cli
          sf plugins:update
          sudo apt-get update && sudo apt-get install -y jq

      - name: 🔐 Authenticate to Salesforce Org
        run: |
          echo "${{ secrets.ORG_SFDX_URL }}" | sf org login sfdx-url --alias $ORG_ALIAS --set-default --sfdx-url-stdin
          sf org list

      - name: 🧩 Query Profiles and Permission Sets
        run: |
          mkdir -p "$DATA_DIR"
          echo "🔍 Querying profiles and permission sets..."
          sf data query --query "SELECT Id, Name, UserLicense.Name, Description, CreatedDate, LastModifiedDate FROM Profile LIMIT 2000" \
            --target-org "$ORG_ALIAS" --result-format json > "$DATA_DIR/profiles.json" || echo "{}" > "$DATA_DIR/profiles.json"

          sf data query --query "SELECT Id, Name, Description, IsOwnedByProfile, Profile.Name, CreatedDate, LastModifiedDate FROM PermissionSet LIMIT 2000" \
            --target-org "$ORG_ALIAS" --result-format json > "$DATA_DIR/permission_sets.json" || echo "{}" > "$DATA_DIR/permission_sets.json"

      - name: Validate Collected Data
        id: check-data
        run: |
          profile_count=$(jq -r '.result.records | length' "$DATA_DIR/profiles.json" 2>/dev/null || echo 0)
          permset_count=$(jq -r '.result.records | length' "$DATA_DIR/permission_sets.json" 2>/dev/null || echo 0)
          total=$((profile_count + permset_count))

          echo "Profiles: $profile_count, Permission Sets: $permset_count"

          if [ "$total" -gt 0 ]; then
            echo "✅ Found security configuration data"
            echo "has_data=true" >> $GITHUB_OUTPUT
          else
            echo "⚠️ No profiles or permission sets found"
            echo "has_data=false" >> $GITHUB_OUTPUT
          fi

      - name: Upload Security Data
        uses: actions/upload-artifact@v4
        with:
          name: security-profiles-data
          path: ${{ env.DATA_DIR }}
          retention-days: 7

  analyze-security-config:
    name: Analyze Profiles and Permission Sets with AI
    runs-on: ubuntu-latest
    needs: collect-security-data
    if: needs.collect-security-data.outputs.has_data == 'true'
    steps:
      - name: Download Security Artifacts
        uses: actions/download-artifact@v4
        with:
          name: security-profiles-data
          path: data

      - name: Install Ollama
        run: |
          curl -fsSL https://ollama.com/install.sh -o install.sh
          chmod +x install.sh && ./install.sh
          echo "$HOME/.ollama/bin" >> $GITHUB_PATH
          export PATH="$HOME/.ollama/bin:$PATH"
          ollama serve &
          for i in {1..30}; do
            curl -s http://localhost:11434/api/version && break
            sleep 2
          done

      - name: Download Model
        run: |
          MODEL="${{ inputs.model_name }}"
          echo "📥 Downloading Ollama model: $MODEL"
          ollama pull "$MODEL"
          ollama list

      - name: 🔄 Split Large Files into Batches
        run: |
          mkdir -p batches
          jq -c '.result.records[]' data/profiles.json | split -l 25 - batches/profile_
          jq -c '.result.records[]' data/permission_sets.json | split -l 25 - batches/permset_
          echo "✅ Split profiles and permission sets into batches for multi-pass AI analysis."

      - name: Create AI Prompt Template
        run: |
          cat <<EOF > prompt_template.txt
          ROLE: You are a senior Salesforce security architect.
          
          TASK: Analyze Salesforce Profiles and Permission Sets data. Identify risks, compliance issues, and optimization opportunities.
          
          FOLLOW THE STRUCTURED OUTPUT BELOW:
          ## 🚨 Critical Security Findings
          | Severity | Finding | Evidence | Recommendation | Priority | Owner |
          
          ## 👥 Profile Analysis
          | Profile Name | Key Permissions | Risk Level | Notes |
          
          ## 🔑 Permission Sets Analysis
          | Permission Set Name | Key Permissions | Assigned Users | Risk Level | Notes |
          
          ## 🧾 Profiles and Permission Sets Summary
          ### Profiles List
          (List all profile names analyzed here)
          ### Permission Sets List
          (List all permission set names analyzed here)
          
          ## 📋 Compliance & Best Practices
          | Framework | Issue | Evidence | Recommendation |
          
          ## 🧾 Summary
          - Total Profiles in This Batch: <to fill>
          - Total Permission Sets in This Batch: <to fill>
          - Model Used: $MODEL
          EOF

      - name: 🤖 Run AI Analysis per Batch
        run: |
          REPORT_FILE="security_analysis_report.md"
          MODEL="${{ inputs.model_name }}"
          > "$REPORT_FILE"

          echo "# Salesforce Security Analysis Report" >> "$REPORT_FILE"
          echo "_Generated on $(date)_ using model: $MODEL" >> "$REPORT_FILE"
          echo "" >> "$REPORT_FILE"

          for batch in batches/profile_* batches/permset_*; do
            echo "🧠 Analyzing batch: $batch"
            batch_data=$(cat "$batch")
            batch_size=$(echo "$batch_data" | wc -l)
            cat prompt_template.txt <(echo "\n\nBATCH DATA ($batch_size records):\n$batch_data") | ollama run "$MODEL" >> "$REPORT_FILE"
            echo "\n---\n" >> "$REPORT_FILE"
          done

          echo "✅ AI analysis completed. Full report saved to $REPORT_FILE"

      - name: Upload Analysis Report
        uses: actions/upload-artifact@v4
        with:
          name: security-analysis-report
          path: security_analysis_report.md
          retention-days: 14

      - name: 📄 Report Preview
        run: |
          echo "## 🧠 Salesforce Security Analysis Summary" >> $GITHUB_STEP_SUMMARY
          echo "- Model Used: \`${{ inputs.model_name }}\`" >> $GITHUB_STEP_SUMMARY
          echo "- Generated on: $(date)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### 🔍 First 20 lines of the AI Report:" >> $GITHUB_STEP_SUMMARY
          echo '```markdown' >> $GITHUB_STEP_SUMMARY
          head -n 20 security_analysis_report.md >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
