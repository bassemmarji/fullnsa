name: AI-Powered New Data Model Analyzer (Lite-Safe)

on:
  workflow_dispatch:
    inputs:
      org_alias:
        description: "Salesforce org alias"
        required: true
        default: "Full-NSA"
      model_name:
        description: "Ollama model to use"
        required: true
        default: "llama3:latest"

env:
  NODE_VERSION: "20"
  DATA_DIR: "data"  # Simplified for consistency

jobs:
  collect-data:
    name: Collect Salesforce Metadata
    runs-on: ubuntu-latest
    outputs:
      has_data: ${{ steps.check.outputs.has_data }}
    steps:
      - uses: actions/checkout@v4

      - name: Setup Node + Salesforce CLI
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}

      - name: Install Salesforce CLI + jq
        run: |
          sudo apt-get update -y
          sudo apt-get install -y jq
          npm install -g @salesforce/cli

      - name: Authenticate to Salesforce
        run: |
          echo "${{ secrets.ORG_SFDX_URL }}" | sf org login sfdx-url \
            --alias "${{ github.event.inputs.org_alias }}" \
            --sfdx-url-stdin

      - name: Query Salesforce Metadata (Safe Mode)
        run: |
          mkdir -p "$DATA_DIR"
          
          # === DEBUG: Simple count to verify Tooling API access ===
          echo "üîç Debug: Testing Tooling API access..."
          sf data query \
            --use-tooling-api \
            --target-org "${{ github.event.inputs.org_alias }}" \
            --result-format json \
            --query "SELECT count() as TotalEntities FROM EntityDefinition" > "$DATA_DIR/debug_count.json" 2>/tmp/debug_err.log || echo '{"records":[{"TotalEntities":0}]}' > "$DATA_DIR/debug_count.json"
          debug_count=$(jq -r '.records[0].TotalEntities // 0' "$DATA_DIR/debug_count.json")
          echo "üìä Debug: Total EntityDefinitions available: $debug_count (if 0, check perms/org setup)"
          if [ "$debug_count" -eq 0 ]; then
            echo "‚ö†Ô∏è Tooling API returns no data - Action: 1) Re-auth as Admin (sf org login web --alias ${{ github.event.inputs.org_alias }}). 2) Enable features in scratch-def.json (e.g., 'Account'). 3) Recreate org (sf org create scratch ...)"
          fi
          
          # === STEP 1: Query main metadata (excluding relationships) ===
          queries=(
            "objects:SELECT Id,DeveloperName,Label,NamespacePrefix,ExternalSharingModel,InternalSharingModel FROM EntityDefinition LIMIT 2000"
            "fields:SELECT QualifiedApiName,EntityDefinition.DeveloperName,DataType,IsIndexed,IsCalculated,IsNillable,Length,Precision,Scale FROM FieldDefinition WHERE EntityDefinition.DeveloperName != null LIMIT 2000"
            "validation_rules:SELECT ValidationName,EntityDefinition.DeveloperName,Active,ErrorMessage,Description FROM ValidationRule WHERE Active=true LIMIT 2000"
            "triggers:SELECT Name,TableEnumOrId,Status,CreatedDate,LastModifiedDate FROM ApexTrigger LIMIT 2000"
            "flows:SELECT MasterLabel, Definition.DeveloperName, Description, Status, ProcessType, CreatedDate, LastModifiedDate FROM Flow WHERE Status = 'Active' LIMIT 2000"
            "record_types:SELECT Name, SobjectType, Description, IsActive FROM RecordType LIMIT 2000"
          )
      
          for q in "${queries[@]}"; do
            name="${q%%:*}"
            soql="${q#*:}"
            echo "üîç Querying $name..."
            
            if sf data query \
                --use-tooling-api \
                --target-org "${{ github.event.inputs.org_alias }}" \
                --result-format json \
                --query "$soql" > "$DATA_DIR/${name}.json" 2>/tmp/sf_err_${name}.log; then
              echo "‚úÖ $name: success"
              echo "üìä $name totalSize: $(jq -r '.totalSize // 0' "$DATA_DIR/${name}.json")"
            else
              echo "‚ö†Ô∏è  $name: failed ‚Äì $(cat /tmp/sf_err_${name}.log)"
              echo '{"totalSize":0,"done":true,"records":[]}' > "$DATA_DIR/${name}.json"
              echo "üìä $name totalSize: 0 (fallback)"
            fi
            rm -f /tmp/sf_err_${name}.log
          done
      
          # === STEP 2: Query relationship fields (requires EntityDefinitionId) ===
          echo "üîç Querying relationships (per object)..."
          relationships_file="$DATA_DIR/relationships.json"
          tmp_dir="$(mktemp -d)"
      
          # Ensure objects.json exists and has records
          if [ ! -s "$DATA_DIR/objects.json" ] || [ "$(jq -r '.records | length // 0' "$DATA_DIR/objects.json")" -eq 0 ]; then
            echo '{"totalSize":0,"done":true,"records":[]}' > "$relationships_file"
            echo "‚úÖ relationships: no objects found"
          else
            # Loop through each object and query its relationship fields
            while IFS= read -r line; do
              entity_id=$(echo "$line" | cut -d' ' -f1)
              developer_name=$(echo "$line" | cut -d' ' -f2-)
              echo "  ‚Üí $developer_name ($entity_id)"
      
              sf data query \
                --use-tooling-api \
                --target-org "${{ github.event.inputs.org_alias }}" \
                --result-format json \
                --query "SELECT QualifiedApiName, EntityDefinition.DeveloperName, RelationshipName, ReferenceTo FROM FieldDefinition WHERE EntityDefinitionId = '${entity_id}' AND (DataType LIKE 'Lookup(%' OR DataType LIKE 'MasterDetail(%') LIMIT 100" \
                > "$tmp_dir/${entity_id}.json" 2>/dev/null || echo '{"totalSize":0,"done":true,"records":[]}' > "$tmp_dir/${entity_id}.json"
            done < <(jq -r '.records[] | select(.Id and .DeveloperName) | "\(.Id) \(.DeveloperName)"' "$DATA_DIR/objects.json")
      
            # Merge all relationship results
            if compgen -G "$tmp_dir/*.json" > /dev/null; then
              jq -s '{
                totalSize: (map(.totalSize // 0) | add),
                done: true,
                records: (map(.records // []) | flatten)
              }' "$tmp_dir"/*.json > "$relationships_file"
              rel_count=$(jq -r '.totalSize // 0' "$relationships_file")
            else
              echo '{"totalSize":0,"done":true,"records":[]}' > "$relationships_file"
              rel_count=0
            fi
      
            rm -rf "$tmp_dir"
            echo "‚úÖ relationships: completed (totalSize: $rel_count)"
          fi
    
      - name: Validate Data Availability
        id: check
        run: |
          object_count=$(jq -r '.totalSize // 0' "$DATA_DIR/objects.json")
          field_count=$(jq -r '.totalSize // 0' "$DATA_DIR/fields.json")
          echo "Objects: $object_count, Fields: $field_count"
          if [ "$object_count" -gt 0 ] && [ "$field_count" -gt 0 ]; then
            echo "‚úÖ Data model metadata available"
            echo "has_data=true" >> $GITHUB_OUTPUT
          else
            echo "‚ö†Ô∏è No metadata found (check if org has custom objects/fields)"
            echo "has_data=false" >> $GITHUB_OUTPUT
          fi

      - uses: actions/upload-artifact@v4
        if: always()  # Upload even on failure for debugging
        with:
          name: data-model-metadata-lite
          path: ${{ env.DATA_DIR }}
          retention-days: 7

  analyze-ai:
    name: Analyze Data Model (AI)
    runs-on: ubuntu-latest
    needs: collect-data
    if: needs.collect-data.outputs.has_data == 'true'
    steps:
      - uses: actions/download-artifact@v4
        with:
          name: data-model-metadata-lite
          path: ${{ env.DATA_DIR }}

      - name: Setup Ollama
        run: |
          curl -fsSL https://ollama.com/install.sh | sh
          ollama serve &
          for i in {1..20}; do
            if curl -s http://localhost:11434/api/version >/dev/null; then
              echo "‚úÖ Ollama API ready"
              break
            fi
            echo "‚è≥ Waiting for Ollama to start ($i/20)..."
            sleep 3
          done
          if [ $i -eq 20 ]; then
            echo "‚ùå Ollama failed to start"
            exit 1
          fi
          ollama pull "${{ github.event.inputs.model_name }}"

      - name: Analyze Data Model with AI
        run: |
          echo "üß† Starting AI Analysis Step..."
          REPORT_FILE="data_model_analysis_$(date +%Y%m%d_%H%M%S).md"
          MODEL="${{ github.event.inputs.model_name }}"
      
          echo "üîç Checking metadata files..."
          ls -lh "$DATA_DIR" || echo "‚ö†Ô∏è No data folder found!"
      
          # Safety: ensure JSON files exist and are readable
          for f in "$DATA_DIR"/*.json; do
            if [ ! -s "$f" ]; then
              echo "‚ö†Ô∏è Missing or empty file: $f"
            else
              echo "‚úÖ Found $f ($(wc -l < "$f") lines)"
            fi
          done
      
          echo "üß© Combining JSON metadata..."
          jq -s 'def safe(r): if r.result.records then r.result.records else (r.records // []) end;
           { "objects": safe(.[0]), "fields": safe(.[1]), "relationships": safe(.[2]),
             "validation_rules": safe(.[3]), "triggers": safe(.[4]), "flows": safe(.[5]),
             "record_types": safe(.[6]) }' \
                "$DATA_DIR"/objects.json "$DATA_DIR"/fields.json "$DATA_DIR"/relationships.json \
                "$DATA_DIR"/validation_rules.json "$DATA_DIR"/triggers.json "$DATA_DIR"/flows.json \
                "$DATA_DIR"/record_types.json > "$DATA_DIR/full_metadata.json"
          
          record_count=$(jq '[.objects, .fields, .relationships, .validation_rules, .triggers, .flows, .record_types] | map(length) | add' "$DATA_DIR/full_metadata.json")
          if [ "$record_count" -eq 0 ]; then
            echo "‚ùå No metadata found in full_metadata"
            jq . "$DATA_DIR/full_metadata.json" | head -n 40
            exit 1
          fi
          echo "‚úÖ Metadata contains $record_count records"
      
          echo "üß† Sending prompt to Ollama..."
          SYSTEM_PROMPT='You are a senior Salesforce Data Architect. Be concise and factual. Base every point ONLY on the JSON.'
          USER_PROMPT=$(cat << 'EOF'
          Analyze the provided Salesforce metadata JSON and produce exactly these sections:

          ## üîç Key Findings
          ## üèõ Object Modeling & Relationships
          ## üöÄ Performance & Indexing
          ## üîê Security & Governance
          ## ‚úÖ Recommendations

          Use bullet points only, no paragraphs. If data is sparse, write "Insufficient data".
          ### METADATA INPUT ###
          EOF
          )
          echo "$USER_PROMPT" > prompt.txt
          jq -c '.' "$DATA_DIR/full_metadata.json" >> prompt.txt
      
          echo "# Salesforce Data Model Analysis Report" > "$REPORT_FILE"
          echo "**Model:** $MODEL" >> "$REPORT_FILE"
          echo "**Generated:** $(date)" >> "$REPORT_FILE"
          echo "" >> "$REPORT_FILE"
      
          ollama_response=$(curl -s http://localhost:11434/api/generate \
            -H "Content-Type: application/json" \
            -d "{
              \"model\": \"$MODEL\",
              \"prompt\": \"$(jq -Rs . < prompt.txt)\",
              \"system\": \"$SYSTEM_PROMPT\",
              \"options\": {\"temperature\": 0.2, \"top_p\": 0.9},
              \"stream\": false
            }" | jq -r '.response // empty')
      
          if [ -z "$ollama_response" ]; then
            echo "‚ùå Ollama returned empty response"
            exit 1
          fi
      
          echo "$ollama_response" >> "$REPORT_FILE"
          echo "‚úÖ AI analysis completed successfully"
    
      - uses: actions/upload-artifact@v4
        with:
          name: ai-data-model-report
          path: data_model_analysis_*.md
          retention-days: 14

      - name: üìÑ AI Summary
        run: |
          echo "## üß† AI Data Model Summary" >> $GITHUB_STEP_SUMMARY
          echo "- Model: \`${{ github.event.inputs.model_name }}\`" >> $GITHUB_STEP_SUMMARY
          echo "- Generated report successfully" >> $GITHUB_STEP_SUMMARY
