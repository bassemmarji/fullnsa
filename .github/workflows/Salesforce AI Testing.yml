name: Salesforce AI Testing

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]
  workflow_dispatch:
    inputs:
      test_scope:
        type: choice
        description: 'Test Scope'
        options:
          - 'full'
          - 'changed-files'
          - 'specific-class'
        default: 'changed-files'
        required: true
      class_name:
        type: string
        description: 'Apex Class Name (required if test_scope is specific-class)'
        required: false
      generate_ai_tests:
        type: boolean
        description: 'Generate AI-powered test classes'
        default: false
        required: false

permissions:
  contents: read
  actions: write
  checks: write
  pull-requests: write

env:
  ORG_ALIAS: test_org
  TEST_DIR: ai-tests
  NODE_VERSION: '20'
  METADATA_DIR: force-app/main/default

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  ai_testing:
    runs-on: ubuntu-latest
    timeout-minutes: 60
    
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}

      - name: Setup Test Environment
        run: |
          # Create necessary directories
          mkdir -p "${{ env.TEST_DIR }}"
          mkdir -p scripts
          
          # Create AI test generator script
          cat > scripts/ai_test_generator.py << 'EOL'
          import os
          import json
          import openai
          from pathlib import Path
          import re
          import sys
          
          class ApexTestGenerator:
              def __init__(self, api_key):
                  if not api_key:
                      raise ValueError("OpenAI API key is required")
                  
                  try:
                      openai.api_key = api_key
                      # Verify API key works
                      openai.Model.list()
                  except Exception as e:
                      raise ValueError(f"Invalid OpenAI API key: {str(e)}")
                  
                  self.test_template = """
          @isTest
          private class {class_name}Test {{
              @testSetup
              static void setup() {{
                  // Setup test data
          {setup_code}
              }}
              
          {test_methods}
          }}
          """
          
              def extract_methods(self, apex_code):
                  """Extract method signatures from Apex code."""
                  method_pattern = r'(public|private|protected|global)\s+(static\s+)?\w+\s+\w+\s*\([^)]*\)'
                  return re.findall(method_pattern, apex_code)
          
              def generate_test_method(self, method_signature, class_code):
                  """Generate a test method using GPT."""
                  try:
                      prompt = f"""
          Generate a comprehensive Apex test method for the following method:
          {method_signature}
          
          Context from the class:
          {class_code}
          
          Requirements:
          1. Include positive and negative test scenarios
          2. Add proper assertions
          3. Follow Salesforce best practices
          4. Include comments explaining the test
          5. Handle bulk testing where appropriate
          6. Test error conditions
          7. Follow test method naming convention: test_methodName_scenario
          """
                      
                      response = openai.ChatCompletion.create(
                          model="gpt-4",
                          messages=[
                              {"role": "system", "content": "You are an expert Salesforce developer specializing in writing high-quality test classes."},
                              {"role": "user", "content": prompt}
                          ],
                          temperature=0.7,
                          max_tokens=2000,
                          timeout=30
                      )
                      
                      return response.choices[0].message.content
                  except Exception as e:
                      print(f"Error generating test method: {str(e)}")
                      return None
          
              def generate_setup_code(self, class_code):
                  """Generate test setup code using GPT."""
                  try:
                      prompt = f"""
          Generate test setup code for the following Apex class:
          {class_code}
          
          Requirements:
          1. Create necessary test data
          2. Handle required relationships
          3. Include comments explaining the setup
          4. Use Test.startTest() and Test.stopTest() where needed
          5. Follow Salesforce best practices
          """
                      
                      response = openai.ChatCompletion.create(
                          model="gpt-4",
                          messages=[
                              {"role": "system", "content": "You are an expert Salesforce developer specializing in test data setup."},
                              {"role": "user", "content": prompt}
                          ],
                          temperature=0.7,
                          max_tokens=1500,
                          timeout=30
                      )
                      
                      return response.choices[0].message.content
                  except Exception as e:
                      print(f"Error generating setup code: {str(e)}")
                      return None
          
              def generate_test_class(self, class_path):
                  """Generate a complete test class for an Apex class."""
                  try:
                      with open(class_path, 'r') as f:
                          class_code = f.read()
                  except Exception as e:
                      print(f"Error reading class file: {str(e)}")
                      return None
                  
                  # Extract class name
                  class_name = Path(class_path).stem
                  
                  # Extract methods
                  methods = self.extract_methods(class_code)
                  if not methods:
                      print(f"Warning: No methods found in {class_path}")
                      return None
                  
                  # Generate setup code
                  setup_code = self.generate_setup_code(class_code)
                  if not setup_code:
                      print("Error: Failed to generate setup code")
                      return None
                  
                  # Generate test methods
                  test_methods = []
                  for method in methods:
                      test_method = self.generate_test_method(method, class_code)
                      if test_method:
                          test_methods.append(test_method)
                  
                  if not test_methods:
                      print("Error: Failed to generate any test methods")
                      return None
                  
                  # Combine into test class
                  test_class = self.test_template.format(
                      class_name=class_name,
                      setup_code=setup_code,
                      test_methods='\n'.join(test_methods)
                  )
                  
                  return test_class
          
          def main():
              try:
                  # Get OpenAI API key from environment
                  api_key = os.getenv('OPENAI_API_KEY')
                  if not api_key:
                      print("Error: OPENAI_API_KEY environment variable not set")
                      sys.exit(1)
                  
                  generator = ApexTestGenerator(api_key)
                  
                  # Get class path from command line args
                  if len(sys.argv) < 2:
                      print("Usage: python ai_test_generator.py <path_to_apex_class>")
                      sys.exit(1)
                  
                  class_path = sys.argv[1]
                  test_class = generator.generate_test_class(class_path)
                  
                  if test_class:
                      # Write test class to file
                      test_path = Path(class_path).parent / f"{Path(class_path).stem}Test.cls"
                      try:
                          with open(test_path, 'w') as f:
                              f.write(test_class)
                          print(f"✅ Generated test class at: {test_path}")
                      except Exception as e:
                          print(f"Error writing test class: {str(e)}")
                          sys.exit(1)
                  else:
                      print("❌ Failed to generate test class")
                      sys.exit(1)
                      
              except Exception as e:
                  print(f"Unexpected error: {str(e)}")
                  sys.exit(1)
          
          if __name__ == '__main__':
              main()
          EOL
          
          # Make script executable
          chmod +x scripts/ai_test_generator.py

      - name: Install Dependencies
        run: |
          # Install Salesforce CLI
          npm install @salesforce/cli --global
          
          # Install Python and pip
          sudo apt-get update
          sudo apt-get install -y python3 python3-pip
          
          # Upgrade pip
          python3 -m pip install --upgrade pip
          
          # Install Python dependencies directly
          pip3 install openai>=1.0.0 pytest>=7.0.0 pytest-xdist>=3.0.0 requests>=2.31.0

      - name: Authenticate to Salesforce
        run: |
          echo "${{ secrets.ORG_SFDX_URL }}" | sf org login sfdx-url \
            --alias ${{ env.ORG_ALIAS }} \
            --set-default \
            --sfdx-url-stdin

      - name: Generate AI Test Classes
        if: ${{ github.event.inputs.generate_ai_tests == 'true' }}
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        run: |
          if [ "${{ github.event.inputs.test_scope }}" = "specific-class" ]; then
            if [ -z "${{ github.event.inputs.class_name }}" ]; then
              echo "Error: Class name is required when test_scope is specific-class"
              exit 1
            fi
            
            # Generate test for specific class
            python3 scripts/ai_test_generator.py \
              "${{ env.METADATA_DIR }}/classes/${{ github.event.inputs.class_name }}.cls"
          else
            # Get classes to generate tests for
            if [ "${{ github.event.inputs.test_scope }}" = "changed-files" ]; then
              # Get changed Apex classes (excluding test classes)
              CHANGED_FILES=$(git diff --name-only ${{ github.event.before }} ${{ github.sha }} | grep -E ".*\.cls$" | grep -v "Test\.cls$")
            else
              # Get all Apex classes (excluding test classes)
              CHANGED_FILES=$(find "${{ env.METADATA_DIR }}/classes" -name "*.cls" | grep -v "Test\.cls$")
            fi
            
            # Generate tests for each class
            for class_file in $CHANGED_FILES; do
              echo "Generating test for: $class_file"
              python3 scripts/ai_test_generator.py "$class_file"
            done
          fi

      - name: Deploy Generated Tests
        if: ${{ github.event.inputs.generate_ai_tests == 'true' }}
        run: |
          # Deploy the generated test classes
          sf project deploy start \
            --source-dir "${{ env.METADATA_DIR }}/classes" \
            --target-org ${{ env.ORG_ALIAS }} \
            --test-level RunLocalTests

      - name: Generate and Run Tests
        id: test-execution
        run: |
          mkdir -p "${{ env.TEST_DIR }}"
          
          # Run tests based on scope
          if [ "${{ github.event.inputs.test_scope }}" = "specific-class" ]; then
            if [ -z "${{ github.event.inputs.class_name }}" ]; then
              echo "Error: Class name is required when test_scope is specific-class"
              exit 1
            fi
            
            sf apex run test \
              --target-org ${{ env.ORG_ALIAS }} \
              --class-names "${{ github.event.inputs.class_name }}" \
              --code-coverage \
              --detailed-coverage \
              --result-format json \
              --output-dir "${{ env.TEST_DIR }}" \
              --wait 20
          else
            # For full or changed-files scope
            TESTS_TO_RUN=""
            if [ "${{ github.event.inputs.test_scope }}" = "changed-files" ]; then
              # Get changed Apex test classes
              TESTS_TO_RUN=$(git diff --name-only ${{ github.event.before }} ${{ github.sha }} | grep -E ".*Test\.cls$" | sed 's/.*\///g' | sed 's/\.cls$//g' | tr '\n' ',')
            fi
            
            # If no specific tests found or full scope, run all tests
            if [ -z "$TESTS_TO_RUN" ] || [ "${{ github.event.inputs.test_scope }}" = "full" ]; then
              sf apex run test \
                --target-org ${{ env.ORG_ALIAS }} \
                --test-level RunAllTestsInOrg \
                --code-coverage \
                --detailed-coverage \
                --result-format json \
                --output-dir "${{ env.TEST_DIR }}" \
                --wait 20
            else
              sf apex run test \
                --target-org ${{ env.ORG_ALIAS }} \
                --class-names "$TESTS_TO_RUN" \
                --code-coverage \
                --detailed-coverage \
                --result-format json \
                --output-dir "${{ env.TEST_DIR }}" \
                --wait 20
            fi
          fi

      - name: Process Test Results
        if: always()
        run: |
          echo "# Salesforce Test Results Summary" >> $GITHUB_STEP_SUMMARY
          
          if [ -f "${{ env.TEST_DIR }}/test-result.json" ]; then
            # Process test results using Python
            python3 - <<EOF
          import json
          import sys
          
          with open('${{ env.TEST_DIR }}/test-result.json') as f:
              results = json.load(f)
          
          # Overall summary
          summary = results.get('summary', {})
          print(f"## Overall Results", file=sys.stderr)
          print(f"- Total Tests: {summary.get('testsRan', 0)}", file=sys.stderr)
          print(f"- Passed: {summary.get('passing', 0)}", file=sys.stderr)
          print(f"- Failed: {summary.get('failing', 0)}", file=sys.stderr)
          print(f"- Duration: {summary.get('totalTime', 0)}s", file=sys.stderr)
          print("", file=sys.stderr)
          
          # Coverage information
          coverage = results.get('coverage', {})
          print(f"## Code Coverage", file=sys.stderr)
          print(f"- Overall Coverage: {coverage.get('coveragePercentage', 0)}%", file=sys.stderr)
          print(f"- Lines Covered: {coverage.get('coveredLines', 0)}/{coverage.get('totalLines', 0)}", file=sys.stderr)
          print("", file=sys.stderr)
          
          # Class coverage details
          print("## Class Coverage Details", file=sys.stderr)
          print("| Class Name | Coverage % | Lines Covered |", file=sys.stderr)
          print("|------------|------------|---------------|", file=sys.stderr)
          
          for cls, data in coverage.get('coverage', {}).items():
              print(f"| {cls} | {data.get('percentage', 0)}% | {data.get('coveredLines', 0)}/{data.get('totalLines', 0)} |", file=sys.stderr)
          
          # Failed tests
          failures = [t for t in results.get('tests', []) if t.get('outcome') == 'Fail']
          if failures:
              print("\n## Failed Tests", file=sys.stderr)
              print("| Test Name | Error Message |", file=sys.stderr)
              print("|-----------|---------------|", file=sys.stderr)
              for test in failures:
                  print(f"| {test.get('fullName', 'N/A')} | {test.get('message', 'N/A')} |", file=sys.stderr)
          EOF
          else
            echo " No test results found" >> $GITHUB_STEP_SUMMARY
          fi
          
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Report generated on $(date '+%Y-%m-%d %H:%M:%S')" >> $GITHUB_STEP_SUMMARY

      - name: Upload Test Artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: ai-test-results
          path: |
            ${{ env.TEST_DIR }}
            ${{ env.METADATA_DIR }}/classes/*Test.cls
          retention-days: 14
          if-no-files-found: warn
          compression-level: 6
          overwrite: true
          permissions: read-all
