name: AI-Powered Reports Analyzer (Fixed)

on:
  workflow_dispatch:
    inputs:
      org_alias:
        description: "Salesforce org alias"
        required: true
        default: "dev"
      model_name:
        description: "Ollama model"
        required: true
        default: "llama3:latest"

env:
  NODE_VERSION: "20"
  DATA_DIR: "data"

jobs:
  collect-and-analyze-reports:
    name: Collect + Analyze Reports (Hardis + Metadata)
    runs-on: ubuntu-latest
    outputs:
      has_data: ${{ steps.check.outputs.has_data }}

    steps:
      - uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}

      - name: Install common linux packages
        run: |
          sudo apt-get update -y
          sudo apt-get install -y jq curl unzip

      # Install modern sf CLI globally (used for auth + queries)
      - name: Install sf (Salesforce CLI)
        run: |
          npm install -g @salesforce/cli@latest
          sf --version

      # Install local sfdx-cli + sfdx-hardis (used only for Hardis analysis via npx)
      - name: Install local sfdx-cli and sfdx-hardis (for Hardis only)
        run: |
          mkdir -p toolchain
          cd toolchain
          npm init -y >/dev/null
          npm install sfdx-cli sfdx-hardis >/dev/null
          npx sfdx --version
          cd -

      - name: Create data directory
        run: mkdir -p "$DATA_DIR"

      # -----------------------------------------------------------
      # AUTHENTICATE using sf (modern client) â€” required for queries
      # -----------------------------------------------------------

      - name: Authenticate to Salesforce (sf)
        run: |
          echo "${{ secrets.ORG_SFDX_URL }}" | sf org login sfdx-url \
            --alias "${{ github.event.inputs.org_alias }}" \
            --sfdx-url-stdin

      - name: Capture org info (instance url + user)
        run: |
          sf org display --target-org "${{ github.event.inputs.org_alias }}" --json > "$DATA_DIR/org_display.json"
          jq -r '.result.instanceUrl // .instanceUrl // ""' "$DATA_DIR/org_display.json" > "$DATA_DIR/instance_url.txt" || true

      # -----------------------------------------------------------
      # METADATA COLLECTION (use sf for queries)
      # -----------------------------------------------------------

      - name: Query Reports Metadata (sf)
        run: |
          sf data query \
            --target-org "${{ github.event.inputs.org_alias }}" \
            --result-format json \
            --query "SELECT Id, DeveloperName, Name, NamespacePrefix, Description, Format, FolderName, LastRunDate, LastViewedDate, LastReferencedDate, CreatedDate, CreatedBy.Name, LastModifiedDate, LastModifiedBy.Name FROM Report LIMIT 5000" \
            > "$DATA_DIR/reports.json" || true

      - name: Query Dashboard Usage (DashboardComponent)
        run: |
          sf data query \
            --target-org "${{ github.event.inputs.org_alias }}" \
            --result-format json \
            --query "SELECT Id, DashboardId, CustomReportId FROM DashboardComponent LIMIT 5000" \
            > "$DATA_DIR/dashboard_components.json" || true

      # -----------------------------------------------------------
      # HARDIS ANALYSIS using npx sfdx (sfdx-cli installed locally)
      # -----------------------------------------------------------

      - name: Run Hardis Report Analyzer (using npx sfdx)
        run: |
          mkdir -p "$DATA_DIR"
          # Use npx to ensure we run the local sfdx binary and avoid replacing global sf
          set -o pipefail

          # Attempt Hardis analysis; capture stdout/stderr
          npx sfdx hardis:org:report:analyze \
            --target-org "${{ github.event.inputs.org_alias }}" \
            --format json \
            --outputfile "$DATA_DIR/hardis_report.json" 2> "$DATA_DIR/hardis_raw_err.log" || true

          # If file not produced or invalid JSON, write a safe fallback
          if [ ! -f "$DATA_DIR/hardis_report.json" ] || [ ! -s "$DATA_DIR/hardis_report.json" ] || ! jq empty "$DATA_DIR/hardis_report.json" >/dev/null 2>&1; then
            echo '{"status":"skipped","findings":[],"summary":{}}' > "$DATA_DIR/hardis_report.json"
          fi

      # -----------------------------------------------------------
      # CHECK FOR DATA
      # -----------------------------------------------------------

      - name: Check if reports exist
        id: check
        run: |
          if [ ! -f "$DATA_DIR/reports.json" ] || [ ! -s "$DATA_DIR/reports.json" ]; then
            echo "has_data=false" >> $GITHUB_OUTPUT
            exit 0
          fi

          # Try multiple possible JSON shapes and count records robustly
          count=$(jq -r 'if has("result") and (.result | has("records")) then .result.records | length elif has("records") then .records | length else 0 end' "$DATA_DIR/reports.json" 2>/dev/null || echo 0)
          if [ "$count" -gt 0 ]; then
            echo "has_data=true" >> $GITHUB_OUTPUT
          else
            echo "has_data=false" >> $GITHUB_OUTPUT
          fi

      - uses: actions/upload-artifact@v4
        with:
          name: raw-and-hardis-data
          path: ${{ env.DATA_DIR }}
          retention-days: 7

  analyze-with-ai:
    name: AI-Powered Insight Generation
    runs-on: ubuntu-latest
    needs: collect-and-analyze-reports
    if: needs.collect-and-analyze-reports.outputs.has_data == 'true'

    steps:
      - uses: actions/checkout@v4

      - uses: actions/download-artifact@v4
        with:
          name: raw-and-hardis-data
          path: ${{ env.DATA_DIR }}

      # -----------------------------------------------------------
      # Setup Ollama (unchanged)
      # -----------------------------------------------------------

      - name: Setup Ollama
        run: |
          curl -fsSL https://ollama.com/install.sh | sh
          ollama serve & disown || true
          for i in {1..20}; do
            if curl -s http://localhost:11434/api/version >/dev/null; then break; fi
            sleep 2
          done
          ollama pull "${{ github.event.inputs.model_name }}" || true

      # -----------------------------------------------------------
      # ENHANCED DATA PREPROCESSING (with more robust jq)
      # -----------------------------------------------------------

      - name: Preprocess Data with Risk Scoring
        run: |
          set -euo pipefail

          # Normalize dashboard components and compute dashboard usage safely (handle nulls)
          jq -r '.result.records // .records // []' "$DATA_DIR/dashboard_components.json" | jq -s 'map(select(.CustomReportId != null and .CustomReportId != "")) | group_by(.CustomReportId) | map({reportId: .[0].CustomReportId, dashboardCount: length})' > "$DATA_DIR/dashboard_usage.json" || echo '[]' > "$DATA_DIR/dashboard_usage.json"

          # Ensure reports array exists
          jq -r '.result.records // .records // []' "$DATA_DIR/reports.json" > "$DATA_DIR/reports_array.json"

          # Enrich reports: robust date parsing (handle multiple timezone formats)
          jq -s '
            (.[0]) as $reports |
            (.[1] // []) as $dash |
            $reports | map(
              . as $r |
              ($dash | map(select(.reportId == $r.Id)) | .[0].dashboardCount // 0) as $dashCount |

              # Recency Score (0-3)
              (if ($r.LastRunDate // null) != null then 3
               elif ($r.LastViewedDate // null) != null then 2
               elif ($r.LastReferencedDate // null) != null then 1
               else 0 end) as $recency |

              # Description Quality Score (0-2)
              (if (($r.Description // "") | length) > 100 then 2
               elif (($r.Description // "") | length) > 0 then 1
               else 0 end) as $descQuality |

              # Dashboard Usage Score (0-3)
              (if $dashCount >= 5 then 3
               elif $dashCount >= 2 then 2
               elif $dashCount >= 1 then 1
               else 0 end) as $dashScore |

              ($recency + $descQuality + $dashScore) as $riskScore |

              # Determine last activity date string either from LastRunDate, LastViewedDate, LastReferencedDate
              ($r.LastRunDate // $r.LastViewedDate // $r.LastReferencedDate) as $lastActivityStr |

              # Convert to epoch days; attempt to normalize timezone like ".000+0000" -> "Z"
              (if $lastActivityStr then
                ($lastActivityStr | sub("\\.\\d{3}\\+([0-9]{4})$"; "Z") | sub("\\+00:00$"; "Z") | fromdateiso8601) as $lastDate |
                ((now - $lastDate)/86400 | floor)
               else 9999 end) as $daysSinceActivity |

              $r + {
                dashboardUsage: $dashCount,
                recencyScore: $recency,
                descriptionQuality: $descQuality,
                dashboardScore: $dashScore,
                healthScore: $riskScore,
                daysSinceLastActivity: $daysSinceActivity,
                riskLevel: (
                  if $riskScore >= 7 then "LOW"
                  elif $riskScore >= 4 then "MEDIUM"
                  else "HIGH" end
                )
              }
            )
          ' "$DATA_DIR/reports_array.json" "$DATA_DIR/dashboard_usage.json" > "$DATA_DIR/reports_enriched.json"

          # Produce category lists (safe fallbacks)
          jq 'map(select(.healthScore <= 3)) | sort_by(.healthScore, .daysSinceLastActivity) | reverse | .[0:30]' "$DATA_DIR/reports_enriched.json" > "$DATA_DIR/high_risk_reports.json" || echo '[]' > "$DATA_DIR/high_risk_reports.json"

          jq 'map(select(((.Description // "") == "") and (.LastViewedDate == null and .LastReferencedDate == null and .LastRunDate == null)) ) | .[0:30]' "$DATA_DIR/reports_enriched.json" > "$DATA_DIR/invalid_reports.json" || echo '[]' > "$DATA_DIR/invalid_reports.json"

          jq 'map(select(.daysSinceLastActivity > 180)) | sort_by(.daysSinceLastActivity) | reverse | .[0:30]' "$DATA_DIR/reports_enriched.json" > "$DATA_DIR/stale_reports.json" || echo '[]' > "$DATA_DIR/stale_reports.json"

          jq 'sort_by(.healthScore, -.daysSinceLastActivity) | reverse | .[0:30]' "$DATA_DIR/reports_enriched.json" > "$DATA_DIR/top_reports.json" || echo '[]' > "$DATA_DIR/top_reports.json"

          jq 'map(select(.dashboardUsage == 0)) | sort_by(.daysSinceLastActivity) | reverse | .[0:30]' "$DATA_DIR/reports_enriched.json" > "$DATA_DIR/orphaned_reports.json" || echo '[]' > "$DATA_DIR/orphaned_reports.json"

          jq 'sort_by(.Format) | group_by(.Format) | map({format: .[0].Format, count: length, avgHealthScore: (map(.healthScore) | add / length | floor), reports: sort_by(.healthScore) | .[0:5] | map({name: .Name, id: .Id, healthScore: .healthScore, folder: .FolderName})})' "$DATA_DIR/reports_enriched.json" > "$DATA_DIR/reports_by_category.json" || echo '[]' > "$DATA_DIR/reports_by_category.json"

          # HARDIS findings extraction (Hardis JSON uses root.findings)
          if [ -f "$DATA_DIR/hardis_report.json" ] && jq -e 'has("findings")' "$DATA_DIR/hardis_report.json" >/dev/null 2>&1; then
            jq '.findings // []' "$DATA_DIR/hardis_report.json" > "$DATA_DIR/hardis_findings.json"
          else
            echo '[]' > "$DATA_DIR/hardis_findings.json"
          fi

          # SUMMARY STATISTICS
          jq -n --slurpfile reports "$DATA_DIR/reports_enriched.json" '{
            totalReports: ($reports[0] | length),
            highRisk: ($reports[0] | map(select(.riskLevel == "HIGH")) | length),
            mediumRisk: ($reports[0] | map(select(.riskLevel == "MEDIUM")) | length),
            lowRisk: ($reports[0] | map(select(.riskLevel == "LOW")) | length),
            orphaned: ($reports[0] | map(select(.dashboardUsage == 0)) | length),
            avgHealthScore: (if ($reports[0] | length) > 0 then ($reports[0] | map(.healthScore) | add / length | floor) else 0 end
          }' > "$DATA_DIR/summary_stats.json"

      # -----------------------------------------------------------
      # ENHANCED AI ANALYSIS
      # -----------------------------------------------------------

      - name: AI Analysis with Detailed Tables
        env:
          MODEL: ${{ github.event.inputs.model_name }}
        run: |
          REPORT="salesforce_reports_analysis_$(date +%Y%m%d_%H%M%S).md"

          # Build CONTEXT JSON for AI (includes instance url for links)
          INSTANCE_URL=$(cat "$DATA_DIR/instance_url.txt" 2>/dev/null || echo "https://yourinstance.salesforce.com")

          CONTEXT=$(jq -nc '{
            instanceUrl: $inst,
            summary: $summary[0],
            highRisk: $highRisk[0],
            invalid: $invalid[0],
            stale: $stale[0],
            top: $top[0],
            orphaned: $orphaned[0],
            categories: $categories[0],
            hardis: $hardis[0]
          }' --arg inst "$INSTANCE_URL" \
            --slurpfile summary "$DATA_DIR/summary_stats.json" \
            --slurpfile highRisk "$DATA_DIR/high_risk_reports.json" \
            --slurpfile invalid "$DATA_DIR/invalid_reports.json" \
            --slurpfile stale "$DATA_DIR/stale_reports.json" \
            --slurpfile top "$DATA_DIR/top_reports.json" \
            --slurpfile orphaned "$DATA_DIR/orphaned_reports.json" \
            --slurpfile categories "$DATA_DIR/reports_by_category.json" \
            --slurpfile hardis "$DATA_DIR/hardis_findings.json")

          PROMPT=$(cat << 'EOF'
          You are a Senior Salesforce Governance Analyst creating a detailed governance report.
          Follow the exact format and table columns requested by the pipeline owner. Use the context JSON appended after this prompt.
          EOF
          )

          PAYLOAD=$(jq -n --arg model "$MODEL" --arg prompt "$PROMPT" --argjson context "$CONTEXT" '{model: $model, prompt: ($prompt + "\n\n" + ($context|tostring)), stream: false, options: {temperature: 0.2, num_ctx: 8192}}')

          # Call local Ollama
          RESPONSE=$(curl -s http://localhost:11434/api/generate -H "Content-Type: application/json" -d "$PAYLOAD" | jq -r '.response // "AI analysis failed."')

          {
            echo "# ðŸ“Š Salesforce Reports Governance Report"
            echo ""
            echo "**Generated:** $(date -u '+%Y-%m-%d %H:%M:%S UTC')"
            echo "**Org Alias:** ${{ github.event.inputs.org_alias }}"
            echo "**Analysis Model:** ${{ github.event.inputs.model_name }}"
            echo ""
            echo "---"
            echo ""
            echo "$RESPONSE"
            echo ""
            echo "---"
            echo ""
            echo "## Raw Data Files"
            echo ""
            echo "- \`reports_enriched.json\` - Full dataset with health scores"
            echo "- \`high_risk_reports.json\` - High risk reports"
            echo "- \`invalid_reports.json\` - Invalid/incomplete reports"
            echo "- \`stale_reports.json\` - Reports inactive for 180+ days"
            echo "- \`orphaned_reports.json\` - Reports not used in dashboards"
            echo ""
          } > "$REPORT"

      - uses: actions/upload-artifact@v4
        with:
          name: salesforce-reports-analysis
          path: salesforce_reports_analysis_*.md
          retention-days: 14

      - uses: actions/upload-artifact@v4
        with:
          name: processed-report-data
          path: ${{ env.DATA_DIR }}/*.json
          retention-days: 14
